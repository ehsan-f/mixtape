% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mix_gcs_write_arrow_dataset-function.R
\name{mix_gcs_write_arrow_dataset}
\alias{mix_gcs_write_arrow_dataset}
\title{Write parquet dataset to Google Cloud Storage using Arrow}
\usage{
mix_gcs_write_arrow_dataset(
  df,
  bucket,
  prefix,
  partitioning = NULL,
  min_files = 100,
  object_format = "parquet",
  basename_template = "part-{i}",
  compression = "snappy"
)
}
\arguments{
\item{df}{Data frame to write}

\item{bucket}{Name of the Google Cloud Storage bucket}

\item{prefix}{Path to destination folder}

\item{partitioning}{Column names to partition by (default: NULL)}

\item{min_files}{Minimum number of files to create (default: 100)}

\item{object_format}{Format of the file to read ('parquet', 'csv', 'rds') (default: 'parquet')}

\item{basename_template}{Template for output file names (default: "part-{i}.parquet")}

\item{compression}{Compression algorithm ('snappy', 'gzip', 'zstd', 'none') (default: 'snappy')}
}
\description{
Writes a data frame as a parquet dataset to Google Cloud Storage using arrow's write_dataset functionality.
Supports partitioning and automatic file splitting based on minimum number of files.
}
